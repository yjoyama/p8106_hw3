---
title: "Homework 3"
author: "Yuki Joyama"
date: "2024-04-03"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)

library(tidyverse)
library(ggplot2)
library(rsample)
library(caret)
library(vip)
library(pROC)
library(MASS)

# setup plot theme
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
```

```{r data}
# data prep
df = read_csv("auto.csv") 
```

First, I will split the dataset into two parts: training data (70%) and test data (30%)
```{r datasplit}
set.seed(1995)
data_split = initial_split(df, prop = .70)

# training data
df_train = training(data_split) 

# test data
df_test = testing(data_split)

# set up 10-fold CV
ctrl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)
```

# (a) 
In this section, I will fit an elastic net model as a penalized logistic regression.

```{r enet}
set.seed(1995)

# find tuning parameter by CV
enet.fit <- 
  train(
    x = df_train[1:7],
    y = df_train$mpg_cat,
    data = df_train,
    method = "glmnet",
    metric = "ROC",
    tuneGrid = expand.grid(
      alpha = seq(0, 1, length = 20),
      lambda = exp(seq(-3, 10, length = 100))
    ),
    trControl = ctrl
  )

# check the best tuning parameter
enet.fit$bestTune

# plot RMSE, lambda and alpha
myCol <- rainbow(25)
myPar <- list(
  superpose.symbol = list(col = myCol),
  superpose.line = list(col = myCol)
)

plot(enet.fit, par.settings = myPar, xTrans = log)

# coefficients in the final model
coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)
```

10-fold cross validation is implemented to select the optimal tuning parameters ($\alpha =$ `r round(enet.fit$bestTune[1], 2)`, $\lambda =$ `r round(enet.fit$bestTune[2], 2)`).  
The model includes five predictors. `acceleration` was found to be redundant in this model.

# (b)
Setting a probability threshold to 0.5 and determine the class labels. If $Pr(Y = \text{low}|X)<0.5$, we will classify this as `low`, otherwise `high`.
```{r}
test.pred.prob <- predict(enet.fit, newdata = df_test, type = "prob")
test.pred <- rep("high", nrow(df_test))
test.pred[test.pred.prob[1] < 0.5] <- "low"
```

The confusion matrix using the test data is as follows.
```{r}
confusionMatrix(
  data = as.factor(test.pred),
  reference = as.factor(df_test$mpg_cat)
)
```

Given the confusion matrix with `high` being the `Positive Class`, we can see that:  
True Positive (TP) = 50  
True Negative (TN) = 57  
False Positive (FP) = 9  
False Negative (FN) = 2  

The metric Accuracy ($\frac{TP+TN}{TP+FP+TN+FN}$) signifies that 90.68% of the samples were correctly classified out of all the samples. Sensitivity ($\frac{TP}{TP+FN}$) indicates that out of all actual Positive Class instances, 96.15% were correctly predicted as `high`. On the other hand, Specificity ($\frac{TN}{FP+TN}$), which is 86.36%, represents the proportion of actual Negative Class instances correctly predicted as `low`.

# (c)
Here, I will train a MARS model using the training data.
```{r mars}
set.seed(1995)

# fit mars model
mars.fit <- train(
  x = df_train[1:7],
  y = df_train$mpg_cat,
  method = "earth",
  tuneGrid = expand.grid(degree = 1:5, nprune = 2:20),
  metric = "ROC",
  trControl = ctrl
)

summary(mars.fit$finalModel)

# best tuning parameters
mars.fit$bestTune

# plot
plot(mars.fit)

# pdp
pdp::partial(mars.fit, pred.var = "displacement", grid.resolution = 200) |> autoplot()

# relative variable importance
vip(mars.fit$finalModel, type = "nsubsets")
```

The best tuning parameters selected from the cross validation is nprune (the upper bound of the number of terms) = `r mars.fit$bestTune[1]` and degree = `r mars.fit$bestTune[2]`.  

The final model can be expressed as the following:  
$\hat{y}$ = `r round(coef(mars.fit$finalModel)[1], 3)` - `r -round(coef(mars.fit$finalModel)[2], 3)` $\times$ h(displacement-232) - `r -round(coef(mars.fit$finalModel)[3], 3)` $\times$ h(year-78) + `r round(coef(mars.fit$finalModel)[4], 3)` $\times$ h(78-year) - `r -round(coef(mars.fit$finalModel)[5], 3)` $\times$ h(3459-weight) + `r round(coef(mars.fit$finalModel)[6], 3)` $\times$ h(displacement-151) + `r round(coef(mars.fit$finalModel)[7], 3)` $\times$ h(horsepower-75) + `r round(coef(mars.fit$finalModel)[8], 3)` $\times$ h(horsepower-133) + `r round(coef(mars.fit$finalModel)[9], 3)` $\times$ h(displacement-262) - `r -round(coef(mars.fit$finalModel)[10], 3)` $\times$ h(horsepower-72) + `r round(coef(mars.fit$finalModel)[11], 3)` $\times$ h(4-cylinders)   
where $h(.)$ is hinge function.  

Now, let's compare the two models.
```{r}
res <- resamples(
  list(
    elastic_net = enet.fit,
    mars = mars.fit
  )
)

summary(res)
bwplot(res, metric = "ROC")
```

Based on the results, the MARS model exhibits a larger median ROC, suggesting improved prediction performance compared to logistic regression.

# (d)
In this section, I will perform linear discriminant analysis using the training data. 
```{r}
lda.fit1 <- lda(mpg_cat ~., data = df_train)
lda.fit1
```

The group means represent the average values of each predictor within each `mpg_cat` group. For example, we can see that in `high` group, the mean number of cylinders is 4.17 whereas in `low` group, it's 6.70. 
The linear discriminant variables are plotted below:  
```{r}
plot(lda.fit1, col = as.numeric(df_train$mpg_cat))
```


# (e)  
```{r}
set.seed(1995)

lda.fit <- train(
  x = df_train[1:7],
  y = df_train$mpg_cat,
  method = "lda",
  metric = "ROC",
  trControl = ctrl
)

res <- resamples(
  list(
    elastic_net = enet.fit,
    mars = mars.fit,
    lda = lda.fit
  )
)

summary(res)
bwplot(res, metric = "ROC")

# test data performance
enet.pred <- predict(enet.fit, newdata = df_test, type = "prob")[,2]
mars.pred <- predict(mars.fit, newdata = df_test, type = "prob")[,2]
lda.pred <- predict(lda.fit, newdata = df_test, type = "prob")[,2]

roc.enet <- roc(df_test$mpg_cat, enet.pred)
roc.mars <- roc(df_test$mpg_cat, mars.pred)
roc.lda <- roc(df_test$mpg_cat, lda.pred)

auc <- c(roc.enet$auc[1], roc.mars$auc[1], roc.lda$auc[1])

modelNames <- c("elastic_net", "mars", "lda")

ggroc(list(roc.enet, roc.mars, roc.lda), legacy.axes = TRUE) +
  scale_color_discrete(labels = paste0(modelNames, "(", round(auc, 3), ")"), name = "Models (AUC)")
```

The resampling results shows that the MARS model has the largest median ROC among the three models.  
Thus, I would prefer to utilize the MARS model for predicting the response variable. Next, I will generate ROC curves for all three models using the test data.  
We can see that the MARS model has the largest AUC of `r round(roc.mars$auc[1], 3)`. 

```{r}
test.pred.prob <- predict(mars.fit, newdata = df_test, type = "raw")

confusionMatrix(
  data = test.pred.prob,
  reference = as.factor(df_test$mpg_cat)
)
```

The misclassification error rate can be obtained by  
1 - accuracy = 1 - 0.9068 = **`r 1 - 0.9068`**





